### 1. Feed-Forward Network
![image](https://user-images.githubusercontent.com/71582504/228065228-21a79023-7e8f-45e5-84bf-8ae50ef4cc06.png)

(5.1)
함수 f(.)는 활성화 함수이다. 
회귀에서는 활성화 함수가 항등 함수이고 분류 문제에서는 시그모이드나 소프트맥스가 사용된다. 
가장 기초적인 형태의 신경망 모델은 다음과 같다. 
![image](https://user-images.githubusercontent.com/71582504/228065340-7627415c-de13-4409-83b1-58a5cf7f1a17.png)

(5.2)

D는 입력벡터 x의 차원이고, j는 특정 레이어의 노드 수이다. (1)은 첫번째 레이어라는 것을 나타낸다. 

$a_j$ 는 activations라고 부른다. 이 값들은 활성함수에 의해 변경되게 된다. 
$$ z_j = h(a_j)  (5.3) $$
이러한 노드를 히든 유닛이라고 부른다. 보통 비선형 함수인 h는 시그모이나 탄젠트 함수를 주로 사용한다. 
최종 출력 결과는 다음과 같다. 
![image](https://user-images.githubusercontent.com/71582504/228065994-27ad8e0f-a15e-430d-9914-5862e1923174.png)

(5.4)
K는 출력할 필드의 개수가 되고 (2) 번 레이어이다. 
여기에 다시 활성 함수를 적용하면 최종 출력값인 y를 얻을 수 있다. 
이 때 사용되는 활성 함수는 시그모이드나 소프트맥스이다. 
지금까지 나온 구조를 종합해보면 다음과 같다. (5.7)=

![image](https://user-images.githubusercontent.com/71582504/228066133-0efdeb69-20bb-4bab-b7d6-b462e7a59726.png)

![image](https://user-images.githubusercontent.com/71582504/228066214-d2ea2483-3e71-44c7-925b-6f511b54941f.png)

--> 위의 그림처럼 신경망은 forward 방향으로 정보를 전달하여 계산한다. 

### 2.MLPy

신경망은 활성화 함수로 시그모이드를 사용하고 퍼셉트론은 Step 함수를 사용한다. 
만약 활성 함수에 선형 함수를 사용하면 히든 유닛이 없는 함수와 동일한 상태가 된다. 
--> 선형 함수의 결합은 다시 선형 함수가 되기 때문에 활성 함수에 선형 함수가 도입되면 최종적으로 $y_k(x, w)$ 함수는 비선형이 아니라 선형 함수가 된다. 
--> 히든 유닛이 없는 신경망은 그냥 선형 분류기가 된다. 

1) sparse network
레이어 간 모든 연결이 존재하는 Dense 네트워가가 보통인데, 일부 노드들만 연결되어 
있는 모델도 있다. 
이런 모델을 Convolutional-NN이라고 하며 이는 5.5.6절에 좀 더 자세히 소개된다. 
![image](https://user-images.githubusercontent.com/71582504/228067347-e04acdad-d2f8-487c-83d2-b0558e079298.png)

이 역시도 feed-forward 방식이다. 

### 5.1.1 weight-space symmetries
feed-forward 네트워크가 가지는 독특한 특성은
서로 다른 w에 대해서도 동일한 입력에 대해 동일한 출력 결과를 만들어낼 수 있다는 것이고 이러한 특성을 symmetry라고 한다. 

1) sign-flip symmetry
예로 히든 유닛에 대한 활성화 함수가 하이퍼볼릭 tanh라고 할 때, 
$$ tanh(-a) = -tanh(a) $$
어떤 히든 유닛 j에 대해 연결되어 입력되는 $w_ji$ 벡터의 부호를 바꾸면 출력 값은 기존 값에 부호만 바뀌게 된다. 
그 다음 레이어에서는 다시 $w_kj$의 부호가 바뀌게 되면 이전과 동일한 결과를 얻게 된다. 
즉, 부호가 반대인 경우에는 두 단계를 거치면 동일한 결과가 만들어지게 된다. 
2-layer에서 히든 유닛이 M개의 경우 이러한 경우를 총 $2^M$개 만들어진다. 

2) Interchange symmetry
히든 유닛 2개에서 서로 위치를 바꾸게 될 때도 최종 출력 값은 바뀌지 않는다. 
그러나 이럴 때에 인덱스 번호는 바뀌게 되기 때문에 w 벡터는 서로 다른 벡터로 취급된다. 
2 - layer에서는 이런 경우를 총 M!개 만들 수 있다. 
1)과 2)를 종합하자면, 대략 $2^M X M!$개가 된다. 

결론적으로 동일한 결과를 가지는 W가 무궁무진하며, 
global minima를 찾을 수 없는 환경에서 동일한 에러 값을 가지는 w 벡터가 다수 존재한다는 것은 무조건 유념해야 하는 사항이다. 

## 5.2 Network Training
지금까지 입력 벡터 x로부터 파라미터를 가지는 비선형 함수를 적용하여 출력 벡터 y를 만드는 신경망에 대해 살펴보았다. 

1) 회귀
회귀 문제를 풀어보자. 
출력 값은 1차원의 실수 값으로 t라고 표기했다. 
t는 x에 종속적인 평균 값을 가지는 가우시안 분포를 따르는 걸 가정하고 들어간다. 
이를 수식으로 표현하면 아래와 같다. (5.12)
![image](https://user-images.githubusercontent.com/71582504/228069730-0c4bfa75-c7d5-40aa-a434-340fc4207f44.png)

가능도 함수는 다음과 같다. 
참고로, 입력값 X 벡터는 서로 독립적으으로 발현되고 출력값 t는 $t_1, ... t_N$이고 1차원 실수값이다. 
![image](https://user-images.githubusercontent.com/71582504/228070006-e23ba962-b4a4-4c7c-a076-76ed8999dbf0.png)

해당 식에 음의 로그항을 이용하면 다음과 같이 에러 함수가 정의된다.(5.13) 
![image](https://user-images.githubusercontent.com/71582504/228070069-a2c5859a-5137-4166-ac74-505c1da2b517.png)

가능도 함수를 최대화하는 문제와 에러 함수를 최소화하는 문제는 동일한 문제인데 
에러 함수를 최소화하는 관점으로 풀어보자. 

따라서 정의한 에러 함수는 다음과 같다. (5.14)
![image](https://user-images.githubusercontent.com/71582504/228070294-ee7f0872-e6d4-408f-8186-b5243c4c6a84.png)

여기서 고려할 점은 $y(x, w)$가 비선형이기 때문에 E(w)가 비볼록(nonconvex)이다. 
이 말은 극대/극소점이 2개 이상 즉, local minima에 빠질 수 있다는 점을 고려해야 한다는 것이다. 

회귀 문제에서의 최종식은 다음과 같다. 
![image](https://user-images.githubusercontent.com/71582504/228070780-91226665-204c-474e-810f-bae7dadf5063.png)
회귀 문제에서 활성 함수는 항등 함수를 사용하므로 $y_k = a_k$가 된다. 
즉, 회귀 문제는 함수 값 자체를 예측하는 문제이므로 활성 함수 $f() = x$이다. 
따라서 다음과 같은 식이 성립된다. (5.18)

![image](https://user-images.githubusercontent.com/71582504/228071004-81556474-6d75-4e9a-8357-8e8b3490566e.png)

2) 이진분류
이진 분류 문제에서는 활성 함수로 시그모이드가 사용된다. (5.19)
![image](https://user-images.githubusercontent.com/71582504/228071266-ce939d68-368c-4ee4-a759-3b3b59ac029b.png)

이진 문제이므로 $p(C_1|x)$ 와 $p(C_2|x)$를 표현해야 하고 이는 y와 1-y로 사용한다.(5.20) 
![image](https://user-images.githubusercontent.com/71582504/228071501-4c33a40b-9741-4db1-ac3f-89f073b85780.png)

또한, 에러 함수는 크로스 엔트로피인데, (5.21)
![image](https://user-images.githubusercontent.com/71582504/228071684-e9a835c3-0bf6-4e21-8c1a-d1ff3cc2f641.png)

분류 문제에서는 노이즈 정확도 $/beta$가 없다. 
왜냐하면 관찰 데이터에 명확한 정답(label)이 결정되어 제공되기 때문이다. 
cross-entropy외에도 사실 sum of squares를 사용해도 상관은 없다만, 
Simard의 논문에 따르면, cross-entropy가 학습 속도도 더 빠르고 성능도 더 좋다고 한다. (5.23)
![image](https://user-images.githubusercontent.com/71582504/228072065-532876e7-397a-470a-a7a8-02a68dc81d49.png)

여기서 $y_nk = y_k(x_n, w)$이다. 
해당 함수를 출력값으로 미분한 식은 다음과 같다. 
![image](https://user-images.githubusercontent.com/71582504/228072247-bf956ab8-244e-4dc6-8055-9a730e0d9fab.png)

즉, 회귀나 분류에서는 에러 함수를 출력값으로 미분한 값은 동일하다는 것을 알 수 있다. 

3) 다중 분류
다중 분류의 활성화 함수는 소프트 맥스 함수이다. (5.25)
![image](https://user-images.githubusercontent.com/71582504/228072428-01e8c6a4-1aca-4cc8-bc18-fcd76ef0fcc3.png)

쭉 넘어가서 에러 함수를 $a_k$로 미분한 값은 다음과 같다. 
![image](https://user-images.githubusercontent.com/71582504/228072545-c5a91491-ae45-42a7-8610-ece7f55f85c5.png)

여기서 결국 중요한 것은 
풀고 싶은 문제에 따라 활성화 함수와 에러 함수를 다르게 정의하지만, 
$a_k$에 대한 에러 함수의 미분 값은 $y_k - t_k$로 동일하다. 

이제 아래는 우리가 다 아는 내용을 좀 더 어렵게 풀어 쓰는 것

### 5.2.1 모수 최적화
--> 최종 목적은 global minimum을 찾는 것이지만 현실적으로 정확한 값을 찾기 어렵기 때문에 몇 개의 local minima를 찾아 가장 작은 값을 가지는 지접을 정답으로 간주할 수 있다. 

비선형 연속 함수의 최적화 문제에서 가중치를 구하는 방법은 다음과 같다. 
우선 초기값 w를 정하고, 가중치 공간 내에서 특정 형태의 반복 식을 통해 찾고자 하는 위치를 계속 변경한다. 
또한, 목적함수에 좀 더 부합되는 w를 업데이트하고 이를 계속 반복한다. 

### 5.2.3 기울기 정보 사용하기 
이후 역전파 알고리즘을 사용하기 위해 에러 함수의 기울기를 구하는 식을 사용하게 된다. 
해당 절의 최종 목적은 
그라디언트 정보를 사용하면서 에러 함수의 최솟값을 찾을 때 연산량이 줄어든다. 

### 5.2.4 기울기 감소 최적화
관찰 데이터를 한 번에 사용해서 가중치를 갱신하는 방식을 batch라고 한다. 
또한, 설명한대로 에러 함수를 그라디언트 방식으로 접근하는 경우에 전역이 아니라 지역 최소점을 찾게 된다. 
따라서, 랜덤하게 초기값을 지정한 뒤에 가급적 여러 번 작업을 반복하여 가장 최소가 되는 지점을 선택하여 사용하면 된다. 

1) On-line method
배치 모드가 아닌 데이터를 하나씩 업데이트 하는 on-line방식도 있다. 
입력 데이터를 순차적으로 입력하면서 반복을 수행할 수 있고 임의로 데이터를 선택해가며 반복을 수행할 수도 있다. 
또한, 부분 데이터를 처리하는 mini-batch 방식을 사용할 수도 있다. 
또 다른 장점은 local minima를 탈출할 가능성에 있다. 
전체 데이터 집합에 대한 오류 함수의 임계점은 보통 개별 데이터 포인트에 대해서는 임계점이 아닐 수도 있기 때문이다. 


